{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Humboldt-WI/adams/blob/master/exercises/Ex11-sentiment-analysis.ipynb) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADAMS Tutorial #11: Sentiment Analysis\n",
    "\n",
    "Sentiment analysis is one of the most popular applications for text classification. It is also interesting from a business perspective. For example, many companies have an interest in analyzing text data emerging in social media to understand how consumers value their products, service, brands, etc.\n",
    "\n",
    "The goal of sentiment analysis is to model the polarity of a piece of text, whether it is rather positive or rather negative. We can frame that as a binary classification problem, with labels of one and zero indicating positive or negative sentiment, respectively. That is the approach we will take today. Other options exist and could involve modeling a three-level target (positive, neutral, negative) or a numeric target variable the values of which represent different strengths of polarity (e.g., between +5 and -5). Whenever approaching the sentiment analysis task by supervised learning, we depend on having some data with sentiment labels. That is often the real challenge in practice - where do the labels come from? - and explains why many labeled data sets re-occur in papers.\n",
    "\n",
    "We will look at the preprocessed movie review data set, that we used in the last tutorial; originally gathered and studied by Maas et al. (https://www.aclweb.org/anthology/P11-1015). We will apply different modeling approach to predict review sentiment, from a simple dictionary-based approach over conventional supervised machine learning to several deep learning techniques. Here is the outline of the tutorial.\n",
    " \n",
    " 1. Preliminaries\n",
    " 2. Dictionary-based sentiment analysis\n",
    " 3. Linear classifier with vectorized inputs\n",
    " 4. Deep learning for text classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preliminaries\n",
    "We reuse the data set from the [last tutorial](https://github.com/Humboldt-WI/adams/blob/master/exercises/Ex10_word2vec/Ex10_word2vec.ipynb). To set up our environment we load the IMDB 50K review data set and also the list of cleaned reviews. We then add the cleaned reviews to the data set to have everything at the same place. It is a good idea to examine a few reviews and make sure that the original version and the cleaned version match. Ones this is confirmed, you can safely discard the raw review text to save some memory. Finally, we update the coding of our target variable and encode positive and negative reviews as one and zero, respectively.  \n",
    "\n",
    "The notebook sets a new milestone in terms of demand for computational resources. We recommend running the notebook  in Colab or another cloud-based platform of your choice.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import standard libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Load/save data from/to disk \n",
    "import pickle\n",
    "\n",
    "# Assess sentiment classification models \n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# Working with pre-trained embeddings from Tutorial #10\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models.keyedvectors import Word2VecKeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a global variable to idicate whether the notebook is run in Colab\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "\n",
    "# Configure variables pointing to directories and stored files \n",
    "if IN_COLAB:\n",
    "    # Mount Google-Drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    DATA_DIR = '/content/drive/My Drive/data/'  # adjust to Google drive folder with the data if applicable\n",
    "else:\n",
    "    DATA_DIR = '../../data/' # adjust to the directory where data is stored on your machine (if running the notebook locally)\n",
    "\n",
    "IMDB_50K = 'IMDB-50K-Movie-Review.zip'  # CSV fil with the original IMDB 50K data set\n",
    "CLEAN_REVIEW = 'imdb_clean_full.pkl'   # List with tokenized reviews after standard NLP preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data integration\n",
    "We load the data set with the movie reviews and and their binary sentiment label. We then load the cleaned review data from the [tutorial #10](https://github.com/Humboldt-WI/adams/blob/master/exercises/Ex10_word2vec/Ex10_word2vec.ipynb) and store it in the data frame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the raw review data set\n",
    "df = pd.read_csv(DATA_DIR + IMDB_50K, sep=\",\", encoding=\"ISO-8859-1\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary-encode the target variable\n",
    "df['sentiment'] = df['sentiment'].map({'positive' : 1, 'negative': 0})\n",
    "df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cleaned review text from tutorial #7\n",
    "import pickle\n",
    "with open(DATA_DIR + CLEAN_REVIEW,'rb') as path_name:\n",
    "    reviews = pickle.load(path_name)\n",
    "assert len(reviews)==50000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cleaned reviews are stored as a list of lists. In order to store the cleaned reviews in one column of the data frame, we need to revert the tokenization and create one long string for every review. NLTK provides functions to undo tokenization, as we illustrate below. It is true that our approach is a little inefficient here, since we will need to tokenize the reviews later to put them into Keras; first detokenizing the tokenized data only to tokenize it again later... not very efficient. With all justified critic, following the notebook might be easiest if all the data is stored in a central point, that is our data frame. Hence, we prepare the data in this way and do not worry about efficiency.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Undo the tokenization and put the data into a new column in the data frame.\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "\n",
    "df['review_clean'] = [TreebankWordDetokenizer().detokenize(review) for review in reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All looks good, let's start with our first sentiment model. However, before moving on, it is probably a good idea to save our data frame to disk. After all, the detokenization took a little while and we don't want to have to do it again if something happens to our data frame. Since we used Pickle before, we stick to this library and simply pickle our data frame. To save disk space, we get rid of the original review text before saving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('review', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store data frame to disk\n",
    "file_name = 'ex11_imdb50K_clean.pkl'\n",
    "df.to_pickle(DATA_DIR + file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data frame from disk\n",
    "file_name = 'ex11_imdb50K_clean.pkl'\n",
    "df = pd.read_pickle(DATA_DIR + file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Downsampling the data to increase speed\n",
    "One more thing before moving on. You should decide whether you want to proceed with the full data frame (i.e., 50K reviews) or draw a random sample to decrease the runtime of the following examples. Using all the data is feasible on any descent computer but prepare for a bit of waiting when training our neural networks. Here is a little bit of code to reduce the amount of data. You can also go back to the previous code and *pickle* the sampled data frame to store it for further use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw a random sample of n reviews to increase the speed of the following steps\n",
    "n = 5000\n",
    "ix = np.random.randint(0, high=df.shape[0]-1, size=n)\n",
    "df = df.loc[ix, :]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dictionary-based sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple approach to rate the sentiment of a text is to literally model it as the sum of its parts through the sentiment of each word. AFINN is an English word listed developed by Finn Ã…rup Nielsen. Words scores range from minus five (negative) to plus five (positive). The English language dictionary consists of 2,477 coded words. Note that you will need to install the library before be able to run the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#    !pip install afinn\n",
    "\n",
    "from afinn import Afinn\n",
    "afinn = Afinn(language='en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We look up the sentiment score for each word in turn and sum up the sentiment values over words. Here are a few examples. Quite easy, isn't it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#* Some examples how to rate texts. Larger values indicate stronger positive feelings\n",
    "print(afinn.score(\"What a marvelous evening, the weather is simply delightful. Wonderful!\"))\n",
    "print(afinn.score(\"I am devastated, the donuts are not what they used to be, what a horrendous taste\"))\n",
    "print(afinn.score(\"To be or not to be, that is the question..\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we already have a data frame, why not adding the sentiment score of every review as a new column. This is a nice use case for the *.apply()* function that Pandas data frames support. We score the cleaned version of the review. If you fancy a little exercise, consider to also score the original review text and compare the differences between the two scores. You could then identify reviews where the sentiment scores differ substantially between the original and cleaned text. That might point to some issues in our data preparation, i.e., the cleaning of the review text in [Tutorial #10](https://github.com/Humboldt-WI/adams/blob/master/exercises/Ex10_word2vec/Ex10_word2vec.ipynb) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the Afinn scores to our data frame \n",
    "# Caution: if you use the full data set of 50K reviews, the scoring will take a while.\n",
    "start = time.time()\n",
    "df['afinn_score'] = df['review_clean'].apply(afinn.score)\n",
    "end = time.time()\n",
    "\n",
    "print('Processed {} reviews in {:.0f} sec.'.format(df.shape[0], end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['afinn_score'].describe() # overall rather positive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can treat the sentiment scores as class predictions. Applying a classification cut-off of zero, we posit that every review with a positive score is classified as positive, and negative otherwise. We can then examine the predictive accuracy of the dictionary-based classifier using standard performance measures from the field of binary classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#* Calculate accuracy of the afinn classifer using a cut-off of zero\n",
    "df['yhat_afinn'] = np.where(df['afinn_score']>0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_dict=accuracy_score(df['sentiment'], df['yhat_afinn'])\n",
    "print(\"Accuracy: {:.4f}\".format(score_dict))\n",
    "confusion_matrix(df['sentiment'], df['yhat_afinn'])\n",
    "\n",
    "score_dict=accuracy_score(df['sentiment'], df['yhat_afinn'])\n",
    "print(\"Accuracy: {:.4f}\".format(score_dict))\n",
    "confusion_matrix(df['sentiment'], df['yhat_afinn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems that our dictionary-based classifier is biased toward the positive reviews. Note that your result might differ depending on which data you are using (all reviews, random sample). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Linear classifier with vectorized inputs\n",
    "Before building complex deep-learning based sentiment classifiers, we can estimate a simple logit model and use it as a benchmark. Now that we start estimating models, we also need to partition our data to estimate model performance on a hold-out test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['review_clean'], df['sentiment'], test_size=0.25, random_state=111)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use count vectorization for our baseline model: take the words of each sentence and create a vocabulary of all the unique words in the sentences. This vocabulary can then be used to create a feature vector of word counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#* Transform the review text using a CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(X_train)\n",
    "\n",
    "X_train_countvec = vectorizer.transform(X_train)  \n",
    "X_test_countvec  = vectorizer.transform(X_test)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_countvec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_countvec[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the above data with the original review\n",
    "case = X_train.iloc[0]  \n",
    "case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look-up the index of the first word\n",
    "print('Index of the first word is {}'.format(vectorizer.vocabulary_['dr']))  # enter first word here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know the index of the word in the vocabulary, we can check the 'feature value' in the training data. This will be some number, which is supposed to quantify how often the first word occurred in the review. Then, examining the review text, we should be able to verify correctness of the count. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the word count of that feature\n",
    "X_train_countvec[0, 6932]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also select some index from the above print of `X_train_countvec` and query the corresponding word. That is just another way of checking that the feature value correctly captures how many times a word appeared in the review. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.get_feature_names()[148]  # You can take any feature index from the above print-out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having convinced ourselves that the data is sound, we proceed by estimating a linear classifier. Given the high-dimensionality of the data set, which is characteristic for count-based word embeddings, we chose LASSO. We set the  argument solver of the linear model to `liblinear`, which is a highly efficient library for regularized linear models, which we can interface via scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate LASSO model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "classifier = LogisticRegression(solver=\"liblinear\", penalty='l1', )\n",
    "classifier.fit(X_train_countvec, y_train)\n",
    "score_lr = classifier.score(X_test_countvec, y_test)\n",
    "\n",
    "# Calculate classification accuracy\n",
    "print(\"Accuracy: {:.4f}\".format(score_lr))\n",
    "confusion_matrix(y_test, classifier.predict(X_test_countvec))  # Note that we do not tune the classification cut-off"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above model is not the best conceivable benchmark. While LASSO is suitable for high-dimensional (text) data, using count vectorization for sentiment analysis is just questionable. TFxIDF weights might be a little better but  suffers the same 'flaw' of considering all the words in the vocabulary as features and counting on the LASSO penalty to filter out the irrelevant words. To make a serious attempt to improve the model, we would need to go back to the data and do some more cleaning (i.e., removing rare and non-sentiment words)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Deep learning for text classification\n",
    "If we can use LASSO, we can also use a neural networks for sentiment prediction. Previous tutorials have already introduced as to different types of models. Time to test them on our review data. However, prior to building models in Keras, we need to do a bit of housekeeping. Our data is not yet in the right form. Keras expects a sequence of integers, which represent (sparse) one-hot-encoded words. So, we have to build our vocabulary and need to make sure that we can move seamlessly from words to integers and vice-versa. Last time, we developed corresponding dictionaries from scratch; remember our *word2id* and *id2word* dictionaries from the [W2V tutorial](https://github.com/Humboldt-WI/adams/blob/master/exercises/Ex10_word2vec/Ex10_word2vec.ipynb). To introduce yet more options, we will use Keras functionality today. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#* Build vocabulary using Keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "NUM_WORDS = 2500  # We could use all words as in the LASSO example but this would increase training times substantially\n",
    "\n",
    "# Create tokenizer object and build vocab from the training set\n",
    "tokenizer_obj = Tokenizer(NUM_WORDS, oov_token=1)  # We fit the tokenizer to the training set reviews. The test set might include\n",
    "tokenizer_obj.fit_on_texts(X_train)  # words that are not part of the training data. The argument oov_token ensures that such new words are mapped to the specified index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert training set reviews to sequences of integer values\n",
    "X_tr_int = tokenizer_obj.texts_to_sequences(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr_int[0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After *fitting* the tokenizer, we have access to its internal vocabulary, which was build-up as part of the fitting. For example, we can convert the integer encoded text back to words as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo = [tokenizer_obj.index_word[token] for token in X_tr_int[0][:10]]\n",
    "demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And again back to integers... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[tokenizer_obj.word_index[token] for token in demo]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Keras layers that we will use later expect the input data to have a fixed, pre-defined shape. For example, you might remember the previous LSTM examples in which we had to make sure that our inputs are of the form *samples / timesteps / features*. At present, our reviews differ substantially in length. So, the next task on our todo list is to padd the reviews and ensure a consistent sequence length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#* Determine the maximum review length in the training set\n",
    "max_review_length = max([len(review) for review in X_tr_int])\n",
    "print('The longest review of the training set has {} words.'.format(max_review_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard practice in NLP is to embed words in a vector space. Considering an embedding dimension of, e.g., 100, each word in the input data (i.e., review) will be mapped to a 100 dim vector. Working with a large embedding dimension and long sequences will result in slow training. Since we care more about illustrating concepts than building the best possible sentiment classifier, we will set an upper bound on the text length and pad reviews accordingly. All reviews that are shorter than our upper bound will be padded with zeros. Longer reviews will be truncated. In practice, you would need to experiment carefully whether and how much truncating the data hurts performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upper bound of the review length for padding\n",
    "MAX_REVIEW_LENGTH = 400\n",
    "\n",
    "X_tr_int_pad = pad_sequences(X_tr_int, MAX_REVIEW_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we dealt only with the training data. So it is about time to also process the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode and pad the test data\n",
    "X_ts_int = tokenizer_obj.texts_to_sequences(X_test)  # Due to oov_token argument, new words will be mapped to 1\n",
    "X_ts_int_pad = pad_sequences(X_ts_int, MAX_REVIEW_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structure of the prepared training and test data\n",
    "X_tr_int_pad.shape, y_train.shape, X_ts_int_pad.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to build some neural networks using Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding,GRU, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.initializers import Constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some variables to centralize the configuration of deep learning models\n",
    "NB_HIDDEN = 16\n",
    "EPOCH = 5\n",
    "BATCH_SIZE = 64 #128\n",
    "EMBEDDING_DIM = 50\n",
    "VAL_SPLIT = 0.25  # fraction of the training set used for validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: Basic GRU \n",
    "We begin with a basic GRU. We chose GRU over LSTM because training the former is faster. More importantly, the input to our GRU are the word embeddings, which we obtain from the Keras `Embedding layer`. In our first model, we initialize the embeddings randomly and train them together with the other network parameters (i.e., in the GRU layer). This architecture is a fairly basic approach toward text classification. We advance the model as we go along.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding layer\n",
    "embedding_layer=Embedding(input_dim=NUM_WORDS, \n",
    "                          output_dim=EMBEDDING_DIM, \n",
    "                          input_length=MAX_REVIEW_LENGTH\n",
    "                         )\n",
    "# GRU text classifier\n",
    "model1=Sequential()                        \n",
    "model1.add(embedding_layer)\n",
    "model1.add(GRU(NB_HIDDEN))\n",
    "model1.add(Dense(1, activation=\"sigmoid\"))\n",
    "model1.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1_story = model1.fit(X_tr_int_pad, y_train, batch_size=BATCH_SIZE, epochs=EPOCH, validation_split=VAL_SPLIT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A little bit of infrastructure\n",
    "The GRU was just the first model in a chain of models of increasing sophistication. Sounds promising doesn't it.\n",
    "Since we are about to train more and more networks, we should develop a little bit of infrastructure to work with them. Specifically, for each network, we need to produce test set predictions. Also, we would like to examine the development of the loss during training; e.g., to judge whether increasing the number of epochs would make sense. Last, it would be useful to save trained models to disk. After all, we spent quite some time on training them to making a backup in case something goes wrong with out notebook makes a lot of sense. Let's develop some helper functions for these tasks.\n",
    "\n",
    "##### Helpfer function for model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCORE_BAG = {}  # Dictionary to store the results of different Keras models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diag_model(model, story, x_ts, y_ts, cut_off=0.5):\n",
    "    ''' \n",
    "        Diagnose fitted keras models by plotting results from the\n",
    "        story (e.g., development of training loss) and calculating\n",
    "        classification accuracy on the test set\n",
    "    '''\n",
    "    score = model.evaluate(x_ts, y_ts, verbose=0)\n",
    "    # Confusion matrix\n",
    "    cmat = confusion_matrix(y_ts, model1.predict(x_ts)>cut_off)\n",
    "    print('Test loss:', score[0])\n",
    "    print('Test accuracy: {:.4f}'.format(np.trace(cmat)/np.sum(cmat)))\n",
    "    print('Confusion matrix:')\n",
    "    print(cmat)\n",
    "    \n",
    "    plt.plot(story.history['accuracy'])\n",
    "    plt.plot(story.history['val_accuracy'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc='upper left')\n",
    "    plt.show()\n",
    "    return score\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's demonstrate our helper function in action by inspecting our first Keras model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCORE_BAG.update({'M1' : diag_model(model1, model1_story, X_ts_int_pad, y_test)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save trained network to disk\n",
    "Saving a model is easy enough and does not warrant a helper function. Using *Pickle*, we store models as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_disk = (model1, model1_story)\n",
    "with open(DATA_DIR + 'model1.pkl','wb') as file_name:\n",
    "    pickle.dump(to_disk, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from disk if needed \n",
    "with open(DATA_DIR + 'model1.pkl','rb') as file_name:\n",
    "    model1, model1_story = pickle.load(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2 GRU with pre-trained IMDB embeddings\n",
    "\n",
    "Model #1 used word embeddings but trained these as part of learning the classifier. You can imagine that corresponding embeddings are different from those resulting from a model that is specifically designed to learn embeddings such as Word-to-Vec. Weights in Model #1 including the weights in the embedding matrix were trained to predict review sentiment. Word-to-Vec, on the other hand, solves a different prediction task related to the co-occurrences of words in a pre-defined context window. We have trained corresponding weights in the [previous tutorial](https://github.com/Humboldt-WI/adams/blob/master/exercises/Ex10_word2vec/Ex10_word2vec.ipynb). It is about time to put these embeddings into action. Our second model will be similar to the first one but use the pre-trained embeddings from [Tutorial 10](https://github.com/Humboldt-WI/adams/blob/master/exercises/Ex10_word2vec/Ex10_word2vec.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up the environment\n",
    "The following demo counts on you having available a stored version of 'good' embeddings from [Tutorial 10](https://github.com/Humboldt-WI/adams/blob/master/exercises/Ex10_word2vec/Ex10_word2vec.ipynb). If needed, you can find such embeddings in our Moodle folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained W2V embeddings obtained from the IMDB review data set\n",
    "imdb_index = KeyedVectors.load_word2vec_format(DATA_DIR + 'w2v_imdb_dim50_embeddings.model', binary=False)\n",
    "print('Loaded pre-trained embeddings for {} words.'.format(len(imdb_index.vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-trained embeddings are essentially just a bunch of numbers for individual words. Needless to say, the numbers carry meaning, capturing syntactic and semantic relationships between words, etc. However, what we just loaded is a dictionary-like data structure in which words serve as a key and the value is the pre-trained embedding of that word. Let's illustrate this using the word *movie* as an example.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = imdb_index['movie']\n",
    "print(e.shape)\n",
    "e[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few words on embeddings...\n",
    "\n",
    "At present, we use embeddings that were obtained from the same corpus, namely the IMDB movie review data set, as the one we are working with right now. That is not common. Typically, the pre-training was done on some other - much larger - corpus. Remember that the very purpose of using pre-trained embeddings is that we hope the pre-trained embeddings to embody some information about word relationships that also prove valuable for our task. The larger the pre-trainind corpus the better. \n",
    "\n",
    "Working with two different corpora, that used for pre-training embeddings and that used in the target task, poses some challenges. First, the pre-trained embeddings will include word vectors for words that do not appear in our corpus. That is less of a problem. \n",
    "\n",
    "Second, and more importantly, our corpus will include some words for which we lack an embedding. Addressing this issue in a satisfactory manner is out of the scope of this tutorial. Pre-training an embedding for unknown words might be a way forward. We will apply a rough fix and map out-of-vocabulary words to an embedding vector of zeros. \n",
    "\n",
    "Third, **and most importantly**, our matrix of pre-trained embeddings will function like a lookup table. Remember that the Keras embedding layer will not compute a dot product between a one-hot encoded input word and the embedding matrix because this would be inefficient. Instead, Keras expects to find the embedding of a word with index i in the i'th row of the embedding matrix. Therefore, it is critical that the word indices must match between our pretrained embeddings and our model's embedding matrix.\n",
    "\n",
    "Since we will work with different pre-trained embeddings in what follows, we implement a little helper function to create an embedding matrix for our corpus. We will then use that embedding matrix when creating our next Keras model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_matrix(tokenizer, pretrain, vocab_size):\n",
    "    '''\n",
    "        Helper function to construct an embedding matrix for \n",
    "        the focal corpus based on some pre-trained embeddings.\n",
    "    '''\n",
    "    \n",
    "    dim = 0\n",
    "    if isinstance(pretrain, KeyedVectors) or isinstance(pretrain, Word2VecKeyedVectors):\n",
    "        dim = pretrain.vector_size        \n",
    "    elif isinstance(pretrain, dict):\n",
    "        dim = next(iter(pretrain.values())).shape[0]  # get embedding of an arbitrary word\n",
    "    else:\n",
    "        raise Exception('{} is not supported'.format(type(pretrain)))\n",
    "    \n",
    "    \n",
    "    # Initialize embedding matrix\n",
    "    emb_mat = np.zeros((vocab_size, dim))\n",
    "\n",
    "    # There will be some words in our corpus for which we lack a pre-trained embedding.\n",
    "    # In this tutorial, we will simply use a vector of zeros for such words. We also keep\n",
    "    # track of the words to do some debugging if needed\n",
    "    oov_words = []\n",
    "    # Below we use the tokenizer object that created our task vocabulary. This is crucial to ensure\n",
    "    # that the position of a words in our embedding matrix corresponds to its index in our integer\n",
    "    # encoded input data\n",
    "    for word, i in tokenizer.word_index.items():  \n",
    "        # try-catch together with a zero-initilaized embedding matrix achieves our rough fix for oov words\n",
    "        try:\n",
    "            emb_mat[i] = pretrain[word]\n",
    "        except:\n",
    "            oov_words.append(word)\n",
    "    print('Created embedding matrix of shape {}'.format(emb_mat.shape))\n",
    "    print('Encountered {} out-of-vocabulary words.'.format(len(oov_words)))\n",
    "    return (emb_mat, oov_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embedding weight matrix\n",
    "imdb_weights, _ = get_embedding_matrix(tokenizer_obj, imdb_index, NUM_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_weights[tokenizer_obj.word_index['film'],:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the network\n",
    "The code to build our GRU is actually very similar to the previous one. The only difference lies in the embedding layer where we now use our pre-trained embeddings as weight. One other notable point concerns the argument `trainable`, which we set to false. With this setting, the weights in the embedding matrix will not change. This simplifies the training but might prohibit our network from unfolding its full potential. Guess what will be our next classifier ones we trained this one ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding layer\n",
    "embedding_layer=Embedding(input_dim=NUM_WORDS, \n",
    "                          output_dim=EMBEDDING_DIM, \n",
    "                          input_length=MAX_REVIEW_LENGTH,\n",
    "                          embeddings_initializer=Constant(imdb_weights), #weights to start with, and not touch during training\n",
    "                          trainable=False  # do not update the weights of the embedding matrix\n",
    "                         )\n",
    "# GRU text classifier\n",
    "model2=Sequential()                        \n",
    "model2.add(embedding_layer)\n",
    "model2.add(GRU(NB_HIDDEN))\n",
    "model2.add(Dense(1, activation=\"sigmoid\"))\n",
    "model2.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2_story = model2.fit(X_tr_int_pad, y_train, batch_size=BATCH_SIZE, epochs=EPOCH, validation_split=VAL_SPLIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assess the model\n",
    "SCORE_BAG.update( {'M2': diag_model(model2, model2_story, X_ts_int_pad, y_test) })\n",
    "\n",
    "# And save it \n",
    "to_disk = (model2, model2_story)\n",
    "with open(DATA_DIR + 'model2.pkl','wb') as file_name:\n",
    "    pickle.dump(to_disk, file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3: GRU with pre-trained IMDB embeddings and fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to improve the previous model. Using pre-trained embeddings, maybe even from some other big corpus, makes sense. At the same time, accuracy might raise if we allow our model to adjust the embeddings to our target task. To test this, we will now build a text classifier where the embedding weights are trainable. In addition, we will demonstrate transferring of the GRU weights of the previous model. Using the final weights of Model #2, which were trained, as starting point for this model should raise accuracy. Let's try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#* Extract weights of the GRU layer of the previous model\n",
    "GRUw = model2.layers[1].get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3=Sequential()\n",
    "embedding_layer=Embedding(NUM_WORDS, \n",
    "                         EMBEDDING_DIM,  \n",
    "                         embeddings_initializer=Constant(imdb_weights), #weights to start with, and not nouch during training\n",
    "                         input_length=MAX_REVIEW_LENGTH, \n",
    "                         trainable=True  # Note this difference to our first GRU\n",
    "                         )\n",
    "model3.add(embedding_layer)\n",
    "#model2.add(Dropout(0.2))\n",
    "model3.add(GRU(NB_HIDDEN, weights=GRUw)) #  dropout=0.1, recurrent_dropout=0.1, for recurring unit and recurrant state\n",
    "model3.add(Dense(1, activation=\"sigmoid\"))\n",
    "model3.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model \n",
    "model3_story = model3.fit(X_tr_int_pad, y_train, batch_size=BATCH_SIZE, epochs=EPOCH, validation_split=VAL_SPLIT)\n",
    "\n",
    "# Assess and store the model\n",
    "SCORE_BAG.update( {'M3': diag_model(model3, model3_story, X_ts_int_pad, y_test) })\n",
    "\n",
    "to_disk = (model3, model3_story)\n",
    "with open(DATA_DIR + 'model3.pkl','wb') as file_name:\n",
    "    pickle.dump(to_disk, file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 4: GRU with pre-trained GloVe embeddings\n",
    "Building your own embeddings might not be the smartest thing. In case the vocabulary you will be working with is not too specific, you can always use the pre-trained embeddings that were crafted from very big corpora. GloVe stands for \"Global Vectors for Word Representation\". It's a somewhat popular embedding technique based on factorizing a matrix of word co-occurrence statistics (https://nlp.stanford.edu/projects/glove/). We could equally stick to W2V and download one of the many available pre-trained versions. There is no specific reason for switching to Glove other than trying our something new. Since the data files of pre-trained embeddings can be quite large, we stick to the smallest available version of Glove, which has an embedding dimension of 50.\n",
    "\n",
    "\n",
    "**Warning**\n",
    "Even when using an embedding dimension of just 50, the data that we process in memory gets quite big. You might experience problems when running the code on your own computer (e.g., slow response times). Should you find that running the code on your machine is practically infeasible, a workaround is to not use GloVe embeddings but re-use the embeddings of previous models. Of course, you would not be able to compare the results properly but you could at least run the codes. To do this, you can make use of the *hack*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_weights = imdb_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and skip over the next code cells in which we load the Glove embeddings and create our weight matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GloVe embeddings\n",
    "# Make sure to adjust the path / name to find the file on your hard disk. You can download it from the above URL\n",
    "glove_index = {}\n",
    "with open(DATA_DIR + 'glove.6B.50d.txt', 'r', encoding=\"utf8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        glove_index[word] = coefs\n",
    "\n",
    "print('Found %s word vectors.' % len(glove_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can re-use our helper function to obtain a proper look-up table for Keras. And that is about it. Everything else is in place so that we can straight go on with building our next classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create matrix with Glove embeddings\n",
    "# Caution: this operation may take a long time and consume a lot of memory\n",
    "glove_weights, _ = get_embedding_matrix(tokenizer_obj, glove_index, NUM_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding layer with the pre-trained GloVe weights\n",
    "embedding_layer = Embedding(NUM_WORDS, \n",
    "                         EMBEDDING_DIM,  \n",
    "                         embeddings_initializer=Constant(glove_weights), \n",
    "                         input_length=MAX_REVIEW_LENGTH, \n",
    "                         trainable=False  # we start with frozen weights and relax this choice in model #5\n",
    "                         )\n",
    "\n",
    "model4=Sequential()\n",
    "model4.add(embedding_layer)\n",
    "#model4.add(Dropout(0.25))\n",
    "model4.add(GRU(NB_HIDDEN))\n",
    "#model4.add(Dropout(0.25))\n",
    "model4.add(Dense(1, activation=\"sigmoid\"))\n",
    "model4.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "model4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model \n",
    "model4_story = model4.fit(X_tr_int_pad, y_train, batch_size=BATCH_SIZE, epochs=EPOCH, validation_split=VAL_SPLIT)\n",
    "\n",
    "# Assess and store the model\n",
    "SCORE_BAG.update( {'M4': diag_model(model4, model4_story, X_ts_int_pad, y_test) })\n",
    "\n",
    "to_disk = (model4, model4_story)\n",
    "with open(DATA_DIR + 'model4.pkl','wb') as file_name:\n",
    "    pickle.dump(to_disk, file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 5: Advance Model #4 by allowing a fine-tuning of embeddings\n",
    "Let's see if fine-tuning the GloVe weights helps. Model #5 is equivalent to model #4 but updates the embeddings as part of the training. Similar to the previous example with the IMDB embeddings (i.e., Model #2 c.f. Model #3), we re-use the weights of the GRU layer to start the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the GRU weights from the previous model\n",
    "GRUglo= model4.layers[1].get_weights()\n",
    "\n",
    "# Set up embedding layer\n",
    "embedding_layer = Embedding(NUM_WORDS, \n",
    "                         EMBEDDING_DIM,  \n",
    "                         embeddings_initializer=Constant(glove_weights), \n",
    "                         input_length=MAX_REVIEW_LENGTH, \n",
    "                         trainable=True  # main difference to model previous model\n",
    "                         )\n",
    "model5=Sequential()\n",
    "model5.add(embedding_layer)\n",
    "model5.add(GRU(NB_HIDDEN, weights=GRUglo))#, dropout=0.1, recurrent_dropout=0.1 ))\n",
    "#model5.add(Dropout(0.25))\n",
    "model5.add(Dense(1, activation=\"sigmoid\"))\n",
    "model5.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "model5.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model \n",
    "model5_story = model5.fit(X_tr_int_pad, y_train, batch_size=BATCH_SIZE, epochs=EPOCH, validation_split=VAL_SPLIT)\n",
    "\n",
    "# Assess and store the model\n",
    "SCORE_BAG.update( {'M5': diag_model(model5, model5_story, X_ts_int_pad, y_test) })\n",
    "\n",
    "to_disk = (model5, model5_story)\n",
    "with open(DATA_DIR + 'model5.pkl','wb') as file_name:\n",
    "    pickle.dump(to_disk, file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 6: Bidirectional GRU\n",
    "Ok, we have trained a lot of different models, and to be fair, in terms of programming and how we use Keras, the codes have become quite repetitive. Let's build one last model and conclude. Our last model with be a **bidirectional GRU**. In sentiment analysis, the full text is available when making a prediction. Therefore, bidirectional NLP models are feasible. Given their conceptual advantage of having access to both, left and right context, we would expect them to perform a little better. Let's see whether this holds true for our data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(NUM_WORDS, \n",
    "                         EMBEDDING_DIM,  \n",
    "                         embeddings_initializer=Constant(glove_weights), #weights to start with, and not nouch during training\n",
    "                         input_length=MAX_REVIEW_LENGTH, \n",
    "                         trainable=False  \n",
    "                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A bidirectional layer is actually two layers with the same structure. Both layers take the input step-by-step, one from beginning to end and one from end-to-beginning. The two hidden states at step $t$ are typically merged by concatenating or summing them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Bidirectional\n",
    "\n",
    "model6 = Sequential()\n",
    "model6.add(embedding_layer) #embeddings are trained\n",
    "model6.add(Bidirectional(GRU(NB_HIDDEN), merge_mode=\"concat\"))\n",
    "#model6.add(Dropout(0.25))\n",
    "model6.add(Dense(units=1, activation='sigmoid'))\n",
    "model6.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "print(model6.summary())\n",
    "\n",
    "# Train the model \n",
    "model6_story = model6.fit(X_tr_int_pad, y_train, batch_size=BATCH_SIZE, epochs=EPOCH, validation_split=VAL_SPLIT)\n",
    "\n",
    "# Assess and store the model\n",
    "SCORE_BAG.update( {'M6': diag_model(model6, model6_story, X_ts_int_pad, y_test) })\n",
    "\n",
    "to_disk = (model6, model6_story)\n",
    "with open(DATA_DIR + 'model5.pkl','wb') as file_name:\n",
    "    pickle.dump(to_disk, file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "The tutorial has covered several important concepts in deep learning for NLP and text classification. We developed several deep learning-based text classifiers using Keras and advanced our understanding of word embeddings. We also saw examples of how to use pre-trained word embeddings in downstream tasks, such as sentiment analysis. \n",
    "\n",
    "Although the focus was more on concepts and code examples than on building top-notch sentiment classifiers, let's conclude the tutorial with a brief analysis how the different approaches compared to each other. Note that the best results for the IMDB data set vary from 91 to 94% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add logit model benchmark results to the score dictionary\n",
    "SCORE_BAG.update( {'Logit': [0, score_lr] })\n",
    "\n",
    "# Put everything in a data frame\n",
    "compare = pd.DataFrame(SCORE_BAG, index=['loss', 'accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If needed, you can run this code (after some adjustment) to load saved models from disk \n",
    "# and do the evaluation ex-post\n",
    "score_dic = {}\n",
    "for i in range(1,7):\n",
    "  file = DATA_DIR + 'model' + str(i) + '.pkl'\n",
    "  print(file)\n",
    "  # Load from disk if needed \n",
    "  with open(file,'rb') as f:\n",
    "    model, story = pickle.load(f)\n",
    "    key = 'M' + str(i)\n",
    "    score_dic.update( {key : diag_model(model, story, X_ts_int_pad, y_test) })\n",
    "\n",
    "# Add logit model benchmark results to the score dictionary\n",
    "score_dic.update( {'Logit': [0, score_lr] })\n",
    "\n",
    "# Put everything in a data frame\n",
    "compare = pd.DataFrame(score_dic, index=['loss', 'accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results \n",
    "x = np.arange(len(compare.columns.values))  # the label locations\n",
    "width = 0.35  # the width of the bars\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(x - width/2, compare.loc['loss'], width, label='Loss')\n",
    "rects2 = ax.bar(x + width/2, compare.loc['acc'], width, label='Accuracy')\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel('Scores')\n",
    "ax.set_title('Model cmparison')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(compare.columns.values)\n",
    "ax.legend()\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
