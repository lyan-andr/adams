{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Humboldt-WI/adams/blob/master/exercises/Ex10_word2vec/Ex10_word2vec.ipynb) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADAMS Tutorial #10 Word2Vec\n",
    "\n",
    "The tutorial revisits the famous Word-to-Vec (W2V) model for learning word embeddings. We introduce the Gensim library, which offers a nice interface to train embeddings. In addition, we implement a W2V model on our own using Keras. In case you would like to take it one step further and code everything yourself using just `numpy`, I recommend [Nathan Rooy's post](https://nathanrooy.github.io/posts/2018-03-22/word2vec-from-scratch-with-python-and-numpy/) the codes of which are available from his [GitHub repo](https://github.com/nathanrooy/word2vec-from-scratch-with-python/blob/master/word2vec.py). A re-implementation of his example with a nice Excel demo is available on [Towards Data Science](https://towardsdatascience.com/an-implementation-guide-to-word2vec-using-numpy-and-google-sheets-13445eebd281).\n",
    "\n",
    "#### Here is the outline of the tutorial\n",
    "\n",
    "1. The IMDB Movie Review data set\n",
    "2. Training W2V embeddings using Gensim\n",
    "3. Manual W2V using Keras\n",
    "\n",
    "\n",
    "Let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The IMDB Movie Review data set\n",
    "\n",
    "We use a popular NLP data set consisting of movie reviews posted at [IMDB](https://www.imdb.com/). The data is available in different sizes and shapes (cleaned, raw, ...) on the web. We use a version from Kaggle, which includes 50K reviews and binary labels whether a review is positive or negative. The labels are useful for sentiment analysis, which we will do in our next tutorial. Today, we will not use them and focus exclusively on the text of reviews. You can download the data from Kaggle: https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data integration and cleaning\n",
    "We need a couple of libraries to pre-process the data. Most steps follow the example of our first NLP tutorial. For example, we will again use the `NLTK toolkit` for standard NLP operation. Although not the focus of this tutorial, we also use a library called `Beautiful Soup` which gained a lot of popularity in web-scraping. We use it to deal with html tags that might occur in some of the reviews. So you might need to install the bs4 library before running the folling code.      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Library re provides regular expressions functionality\n",
    "import re\n",
    "\n",
    "# To keep an eye on runtimes\n",
    "import time\n",
    "\n",
    "# Saving and loaded objects\n",
    "import pickle\n",
    "\n",
    "# Library beatifulsoup4 handles html\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Standard NLP workflow\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remeber to adjust the path so that it matches your environment\n",
    "df = pd.read_csv(\"../../data/IMDB-50K-Movie-Review.zip\", sep=\",\", encoding=\"ISO-8859-1\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently, some of the reviews include HTML. So we probably have to do some data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[1, 'review']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data has a nice balanced distribution of positive and negative reviews. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map label\n",
    "df['sentiment'] = df['sentiment'].map({'positive' : 1, 'negative': 0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampling\n",
    "Working with the full data set of 50K reviews is time consuming. For the tutorial, you might want to use a random sample instead. For a modern computer, a sample size of 5000 should be feasible, without increasing the time too much. We will also make available results from using the full data sets, for example a cleaned version of the full data set or word embeddings trained on the full version of the data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw a radnom sample to save time\n",
    "sample_size = 500\n",
    "idx = np.random.randint(0, high=df.shape[0], size=sample_size)\n",
    "df = df.loc[idx,:]\n",
    "\n",
    "df.reset_index(inplace=True, drop=True)  # dropping the index prohibits a reidentification of the cases in the original data frame\n",
    "df.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP pipeline\n",
    "Our NLP workflow is almost the same as in the previous tutorial. We just clean-up the code a little by putting everything into one function *clean_reviews()*. In that function, we will use lemmatization. As you might remember from Tutorial #9, lemmatization supports different forms of a word, e.g., whether it is used as a noun, verb, etc. We implement a little helper function that uses the POS tagger of the NLTK toolkit. This will allow us to select a suitable grammatical form for the lemmatizer. Here is the helper function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize with POS Tag\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character for lemmatization\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the helper function\n",
    "[get_wordnet_pos(x) for x in [\"house\", \"car\", \"go\", \"nice\",\"nicely\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is our function to clean reviews. We implement it in such a way that we receive as input a set of reviews (in our case a Pandas series object), iterate over that set, and pre-process every review. Alternatively, we could have written the function such that it processes a single review. The latter approach would then facilitate calling it *DataFrame.apply()*. Which approach is better is probably a matter of choice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_reviews(df):\n",
    "    \"\"\" Standard NLP pre-processing chain including removal of html tags, non-alphanumeric characters, and stopwords.\n",
    "        Words are subject to lemmatization using their POS tags, which are determind using WordNet. \n",
    "    \"\"\"\n",
    "    reviews = []\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    print('*' * 40)\n",
    "    print('Cleaning {} movie reviews.'.format(df.shape[0]))\n",
    "    counter = 0\n",
    "    for review in df:\n",
    "        \n",
    "        # remove html content\n",
    "        review_text = BeautifulSoup(review).get_text()\n",
    "        \n",
    "        # remove non-alphabetic characters\n",
    "        review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
    "    \n",
    "        # tokenize the sentences\n",
    "        words = word_tokenize(review_text.lower())\n",
    "  \n",
    "        # filter stopwords\n",
    "        words = [w for w in words if w not in stopwords.words(\"english\")]\n",
    "        \n",
    "        # lemmatize each word to its lemma\n",
    "        lemma_words =[lemmatizer.lemmatize(i, get_wordnet_pos(i)) for i in words]\n",
    "    \n",
    "        reviews.append(lemma_words)\n",
    "              \n",
    "        if (counter > 0 and counter % 500 == 0):\n",
    "            print('Processed {} reviews'.format(counter))\n",
    "            \n",
    "        counter += 1\n",
    "        \n",
    "    print('DONE')\n",
    "    print('*' * 40)\n",
    "\n",
    "    return(reviews) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#* Do the cleaning\n",
    "# CAUTION: depending on your data set size, the processing might take a while \n",
    "reviews = clean_reviews(df.review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check all is well\n",
    "print(df.review[0])\n",
    "print(reviews[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the data\n",
    "Should you have used the full data set in the above cleaning, you will want to store your results. The following codes exemplifies the use of a library called `Pickle`, which offers an easy way to store Python objects on your hard disk. The code is pretty self-explanatory. You can also skip over it, e.g. when using only a small sample of reviews. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cleaned reviews using Pickle\n",
    "# 'wb' specifies 'write (open in binary mode)'\n",
    "# binary mode is important on Win for non-text files\n",
    "with open('imdb_clean.pkl','wb') as path_name:\n",
    "    pickle.dump(reviews, path_name)\n",
    "\n",
    "# 'rb' specifies 'read (open in binary mode)'\n",
    "with open('imdb_clean.pkl','rb') as path_name:\n",
    "    reviews = pickle.load(path_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Full cleaned IMDB data set \n",
    "If you do not invest the time to clean the full IMDB data set with its 50K reviews, you can find a clean version on moodle. The following code loads that data set from disk (assuming it is located in your working directory).   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('imdb_clean_full.pkl','rb') as path_name:\n",
    "    reviews = pickle.load(path_name)\n",
    "len(reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bird's eye view\n",
    "Let's have a look what folks talk about in this data set. Using the class *Counter* from the collections package, we can easily count word occurrences and query the most common words. We can also check the number of occurrences for specific words. We do not really need the *word_counter* here and only use it to get a feeling for the data set. However, note that we will use it later on when building a vocabulary for our manual W2V model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through the words and update a counter keeping track of word counts\n",
    "import collections\n",
    "\n",
    "word_counter = collections.Counter()\n",
    "for r in reviews:\n",
    "    for w in r:\n",
    "        word_counter.update({w: 1})\n",
    "        \n",
    "word_counter.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#* Check frequency of some target word\n",
    "word_counter[\"tarantino\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training W2V Embeddings using Gensim\n",
    "When it comes to embeddings, the most typical use case is to **download pre-trained embeddings** and employ these for some downstream tasks (with or without fine-tuning). The Keras *embedding layer* supports that use case very well. We will make use of it in a next tutorial. Another use case is that you want to **train your own embeddings**. Since this tutorial aims at deepening our understanding of W2V, we focus on this use case.\n",
    "\n",
    "*Gensim* is a popular library for text processing. Although maybe even more geared toward topic modeling, it offers, amongst others, implementations of several algorithms to learn word embeddings including *W2V*, *GloVe*, and *Fasttext*. The following demonstrates training W2V embeddings using the IMDB data using Gensim. By the way, you might need to install it ;)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recap W2V\n",
    "Let's quickly revisit the principles of W2V. Please consult the paper of Mikolov et al. (2013) for a detailed description.\n",
    "\n",
    "W2V establishes a word's meaning by the words that frequently appear close-by (distributional semantics). More specifically, the context of a word consists of the words that appear next to it within a pre-defined window (let's say 5 words).\n",
    "\n",
    " - the quality of *air* in mainland China has been decreasing since..\n",
    " - doctors claim the *air* you breath defines the overall wellbeing...\n",
    " - the currents of hot *air* have been bursting from underground\n",
    " - the mountain *air* was crystal clean and filled with ..\n",
    " - in case of *air* supply shortages, the submarine will..\n",
    "\n",
    "Taking the word *air* as our **target word**, the words around *air*, called context words, define the **meaning** of the word *air* in W2V.\n",
    "\n",
    "![w2vprocess](w2v.jpg)\n",
    "<br>\n",
    "inspired by https://www.youtube.com/watch?v=BD8wPsr_DAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Gensim W2V model\n",
    "\n",
    "Training word embeddings using Gensim is very easy. But note that, depending on your data, the code may take quite a while to run. Again, word embeddings trained on the full 50K data set for 500 epochs are available on Moodle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAUTION: Running the code might take a long time\n",
    "from gensim.models import Word2Vec    \n",
    "\n",
    "emb_dim = 100  # embedding dimension\n",
    "# Train a Word2Vec model\n",
    "model = Word2Vec(reviews, \n",
    "                 min_count=1,  #min_count means the frequency benchmark, if =2 and word is used only once - it's not included\n",
    "                 window=5,     #the size of context\n",
    "                 iter=100,     #how many times the training code will run through the data set, same as epochs (first pass is to create dict)\n",
    "                 size=emb_dim, #size of embedding\n",
    "                 workers=2)    #for parallel computing\n",
    "# summarize the loaded model\n",
    "print(model)\n",
    "words=list(model.wv.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input / output handling\n",
    "Gensim supports saving and loading of trained embeddings in two versions. Option 1 allows you do load embeddings and continue training. This is very useful, for example to save temporary results when training real embeddings for many epochs. The disadvantage is that saving/loading takes longer and that the file consumes more disk space. Therefore, when you are done with the training and only want to store the embeddings, you should go for option 2. We showcase both options below. More information are available on the [Gensim homepage](https://radimrehurek.com/gensim/models/word2vec.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option a) save in such a way that you can continue training later\n",
    "embs=\"gensim_movie_embeddings.model\"\n",
    "model.save(embs)\n",
    "\n",
    "# Overwrite variable to show that loading works\n",
    "model = 0\n",
    "# Load model from disk\n",
    "model =  Word2Vec.load(embs)\n",
    "model.wv['nice']  # get one embedding to show that loading worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option b) save only the trained word vectors; continuation of training is not possible but IO speed increases\n",
    "embs=\"w2v_movie_embeddings.model\"\n",
    "save_as_bin = False\n",
    "model.wv.save_word2vec_format(embs, binary=save_as_bin)  # set binary to True to save disk space; false facilitates inspecting the embeddings in a text editor\n",
    "\n",
    "model = 0\n",
    "\n",
    "# Load model from disk\n",
    "from gensim.models import KeyedVectors\n",
    "model = KeyedVectors.load_word2vec_format(embs, binary=save_as_bin)\n",
    "model['nice']  # get one embedding to show that loading worked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with the trained embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a pre-trained version of the embeddings, which were trained on the full IMDB data set for 500 epochs. The examples are inspired by [this Kaggle kernel](https://www.kaggle.com/pierremegret/gensim-word2vec-tutorial). If you want to visualize the trained word vectors have a look at [this post](https://www.kaggle.com/jeffd23/visualizing-word-vectors-with-t-sne). It is fairly easy to create a TSNE visualization but to get meaningful results you would need to prepare the data more carefully; for example removing too frequent words and too infrequent words. Ultimately, working with the embeddings would involve building a proper NLP model and using it to solve some downstream tasks. So maybe the section should better be called playing with embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This data set is available on moodle\n",
    "model = KeyedVectors.load_word2vec_format(\"w2v_imdb_full_500_epocs.model\", binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Which word is most similar to another word?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.most_similar(positive=['bad'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How similar are two words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.similarity('good', 'great')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('How similar is Tarantino to Spielberg: {}'.format(model.similarity('tarantino', 'spielberg')))\n",
    "print('How similar is Emmerich to Spielberg: {}'.format(model.similarity('emmerich', 'spielberg')))\n",
    "\n",
    "print('How similar is Paltrow to Bullock: {}'.format(model.similarity('paltrow', 'bullock')))\n",
    "print('How similar is Paltrow to Alba: {}'.format(model.similarity('paltrow', 'alba')))\n",
    "\n",
    "print('How similar is Cruise to Depp: {}'.format(model.similarity('cruise', 'depp')))\n",
    "print('How similar is Cruise to Willis: {}'.format(model.similarity('cruise', 'willis')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Which word does not fit in?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.doesnt_match(['cool', 'great', 'lovely', 'weak'])\n",
    "model.doesnt_match(['cruise', 'willis', 'pacino', 'reeves'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A is to B as C is to ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.most_similar(positive=['woman', 'spielberg'], negative=['man'], topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phrase detection\n",
    "W2V trains one embedding per word. The model is agnostic of common phrases such as 'New York'. It would train one embedding for new and another for york, provided both words are part of the vocabulary. You can get better embeddings by adding common phrases to the vocabulary. W2V will then train individual embeddings for these phrases. Gensims also comes with a phrase detection models, which allows you to handle bigrams, trigrams and the like. We will not retrain our W2V model but sketch how you can use Gensim to get these common phrases. You could then consider to add (some of) them to your vocab and enhance the model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phrases\n",
    "\n",
    "# Train a bigram model\n",
    "bigram_model = Phrases(reviews, min_count=10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the original review and the version after phrase detection\n",
    "reviews[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_model[reviews[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can again make use of your counter class to examine the most common bigrams in the corpus, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_counter = collections.Counter()\n",
    "for key in bigram_model.vocab.keys():\n",
    "    if key.decode().find('_')>-1: # the decode is needed because Gensims stores keys as bytes\n",
    "        bigram_counter[key] += bigram_model.vocab[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_counter.most_common(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above bigrams might be frequent. However, you would not consider training individual embeddings for phrases such as *look_like* or *waste_time*. This shows how proper phrase detection in the scope of W2V is nontrivial and would require more work before we can hope to get descend results.     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual Word2Vec using Keras\n",
    "\n",
    "In the following, we will re-implement W2V in Keras. Remember that W2V proposes two models for learning word vectors, continuous-bag-of-words (CBOW) and Skip-Gram. IN a nutshell, CBOW predicts a central target word from surrounding context words, while Skip-Gram takes the opposite approach. Given a <font color='red'>target word</font>, predict <font color='green'>context words</font> with high chance to appear next to the target word in a corpus. Considering one of the above example sentences and a widow size of 2, we can highlight target and context words as follows:<br><br>\n",
    "[doctors <font color='green'>claim the</font><font color='red'> air </font><font color='green'>you breath</font> defines]. \n",
    "<br><br>Using a question mark to indicate the target variable of the model, we obtain:\n",
    "\n",
    "[doctors *? ?* **air** *? ?* breath] in Skip-Gram versus  [doctors *claim the* **?** *you breath* defines] in CBOW.\n",
    "\n",
    "\n",
    "In this tutorial, we focus on Skip-Gram, which seems to be the preferred approach in practice. The code is based on a great tutorial by [Dipanjan Sarkar](https://towardsdatascience.com/understanding-feature-engineering-part-4-deep-learning-methods-for-text-data-96c44370bbfa), in which you can also find a Keras implementation of CBOW; if interested. However, as nice as the post is, the code is not compatible with the recent version of Keras, which is the one you probably use (i.e., Keras 2). So we will take care of that issue in our implementation.  \n",
    "\n",
    "\n",
    "Another potentially useful demonstration how to implement the W2V model in Keras is available at [adventuresinmachinelearning.com](https://towardsdatascience.com/understanding-feature-engineering-part-4-deep-learning-methods-for-text-data-96c44370bbfa). The post provides a lot of nice explanations. However, it uses a shared embedding layer for target and context words, which seems to be wrong. There is some more debate concerning code quality and correctness on Reddit. In summary, have a look at the [adventuresinmachinelearning.com] for some additional explanations, but bear in mind that the Keras code seems to be flawed.\n",
    "\n",
    "Before moving on, let's remember the architecture of the skip-gram W2V model.\n",
    "\n",
    "![sg](https://upload.wikimedia.org/wikipedia/commons/9/95/Skip-gram.png)\n",
    "<br>\n",
    "Source: https://upload.wikimedia.org/wikipedia/commons/9/95/Skip-gram.png\n",
    "\n",
    "Given a sentence - better to say sequence of text - we take a target word and predict a set of context words, that is, words, which appear in a certain <font color=\"green\">**context window**</font>  $[w_{-i},\\ldots, w, \\ldots, w_{+i}]$, where $i$ is the *window size* and the number of context words to consider is window size $\\times 2$. \n",
    "\n",
    "An important caveat with the above picture is that a corresponding model would not scale. Remember that the output layer involves a high-dimensional softmax which is too costly to compute for any reasonably sized corpus. Among the two options around this problem, *hierarchical softmax* and *negative sampling*, we will make use of the latter. So given a target word, our prediction task will be to classify whether another word is an actual context word for that target word, or a random word sampled from the corpus according to some probability distribution. This is a binary classification tasks. Thus, the output of our neural network is must cheaper to compute. Instead of a high-dimensional softmax we only need a simple logistic classifier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the vocabulary\n",
    "Let's start with building our vocabulary. It is common practice to not train to train every word but words that occur reasonably frequent. For rare words, training a good embedding is difficult. Remember how this issue motivated subword embeddings like Fasttext. In our example, we simply use the most frequent words from the review corpus and try to compute embeddings for these words. This is the point where our word_counter (see above) comes in handy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is best to start from a clear defined version of the data. The names of files may be different on your machine.\n",
    "# So make sure to adjust the code if you need to. Then, uncomment the line that you need, the small sample or the full one.\n",
    "\n",
    "# This should be the same version of the cleaned data set\n",
    "with open('imdb_clean.pkl','rb') as path_name:\n",
    "    cleaned_reviews = pickle.load(path_name)\n",
    "    \n",
    "# And this should be the full version of 50K cleaned reviews    \n",
    "#with open('imdb_clean_full.pkl','rb') as path_name:\n",
    "#    cleaned_reviews = pickle.load(path_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is copied from above. We run it again to make sure that the \n",
    "# word_counter is adjusted to the right set of cleaned reviews (e.g., small or full)\n",
    "word_counter = collections.Counter()\n",
    "for r in cleaned_reviews:\n",
    "    for w in r:\n",
    "        word_counter.update({w: 1})\n",
    "\n",
    "# Extract the n most common words from the corpus\n",
    "vocab_size = 1000\n",
    "vocab = word_counter.most_common(vocab_size)\n",
    "vocab = [x[0] for x in vocab]\n",
    "vocab[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next task is to build dictionary. For Keras, we need to encode words as integers, which Keras will then interpret as indices into a one-hot vector of the size of the vocabulary. We build two dictionaries. One to map words to their code (i.e., unique integer) and one to revert the mapping and decode words. \n",
    "\n",
    "In the below code, we implicitly exploit the fact that our vocabulary is ordered by frequency. The most frequent word receives the index 1, the second-most frequent word the index two, and so forth. That will prove useful later when calculating sampling weights for the negative sampling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = range(1, vocab_size)\n",
    "word2id = dict(zip(vocab, idx))\n",
    "id2word = dict(zip(idx, vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Vocabulary size: {}'.format(vocab_size))\n",
    "print('Vocabulary Sample:', list(word2id.items())[:10])\n",
    "print(list(word2id.items())[-10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have noted that we have so far left out the index 0. This index is commonly reserved for unknown words, which we map to a special token. Rmember that our vocabulary is not very large when compared to the number of words that exists in a language (e.g., ~300K in English). So there will be a lot of unknown words in a text and we deal with them but mapping every unknown word to the token `UNK`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2id[\"UNK\"] = 0\n",
    "id2word[0] = \"UNK\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to map unknown words to index 'unknown'\n",
    "def encode_review(review, dictionary):\n",
    "    output = []\n",
    "    for word in review:\n",
    "        if word not in dictionary.keys():\n",
    "            output.append(dictionary[\"UNK\"])\n",
    "        else:\n",
    "            output.append(dictionary[word])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to turn our reviews into integer numbers, which is the format that Keras expects, while accounting for unknown words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#* Build the corpus for W2V by encoding the reviews\n",
    "coded_review = []\n",
    "for r in cleaned_reviews:\n",
    "    coded_review.append(encode_review(r, word2id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some testing\n",
    "#print(reviews[0])\n",
    "#print(coded_review[0])\n",
    "if len(coded_review[0]) == len(reviews[0]):\n",
    "    print(\"Looks good\")\n",
    "else:\n",
    "    print(\"that can't be right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate training data\n",
    "Th training data for our skip-gram model consists of tuples (target, context) with corresponding label (0/1), indicating whether the second word really appeared in the context of the target word our not. Fortunately, Keras has a ready-made function that we can use for that purpose. Specifically, the function `skipgrams` takes a sentence as input and outputs:\n",
    "\n",
    "1. target words in combination with a context word\n",
    "2. a label if the context word is from the actual context or randomly sampled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import skipgrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first illustrate the function *skipgrams()* for a single short sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produce a list of review lengths\n",
    "r_lengths  = [len(r) for r in coded_review]\n",
    "# Indices of reviews ordered by their length in ascending order\n",
    "ix = np.argsort(r_lengths)\n",
    "\n",
    "pic = ix[15]  # the shortest review is maybe too short, this is just an arbitrary selection of some short review\n",
    "[(id2word[t], t) for t in coded_review[pic]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_sentence = coded_review[pic]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that a window size of `i` translates to $[w_{-i},\\ldots, w, \\ldots, w_{+i}]$, so the number of context words to consider is window size $\\times 2$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs, labels = sequence.skipgrams(example_sentence, vocabulary_size=vocab_size, window_size=window_size)\n",
    "for i in range(10):\n",
    "    print(\"({:s} ({:d}), {:s} ({:d}))\\t -> {:d}\".format( id2word[pairs[i][0]], pairs[i][0], id2word[pairs[i][1]], pairs[i][1], labels[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above demo, negative samples not appearing in the context window of the target words were picked at random. According to empirical evidence, the probability of a word to be sampled as negative example should be related to its frequency. Otherwise, we might end up with focussing too much on frequent words. Keras provides a utility function, *make_sampling_table*, to calculate sampling weights for each word in the corpus. Details are available in the [Keras documentation](https://keras.io/preprocessing/sequence/). The `sampling_table` is a list of sampling probabilities, one for each word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samp_tab = sequence.make_sampling_table(vocab_size)\n",
    "samp_tab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the increasing magnitude of the sampling weights. Sampling words from the corpus using this sampling distribution requires an that the words in the corpus are ordered by frequency. Remember the idea is that when sampling negative examples we do not want to focus too much on the frequent words; in our case words like 'movie', and 'file', and 'like'. Therefore, we raise the chance of less frequent words to be sampled as negative examples. \n",
    "\n",
    "When building our corpus above, we used the *most_common()* function. Therefore, the words in our corpus are ordered in decreasing order by their frequency. We will make use of our sampling table to govern the sampling of negative examples make generating the training set for our W2V model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAUTION: yet another operation that is not cheap when using all data\n",
    "start = time.time()\n",
    "skip_grams = [sequence.skipgrams(coded_review, vocabulary_size=vocab_size, window_size=window_size, sampling_table=samp_tab) for coded_review in corpus]\n",
    "end = time.time()\n",
    "print('Generated {} skip-grams in {} sec.'.format(len(skip_grams), end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the neural network\n",
    "We are ready to design our NN architecture using Keras. We feed the network with pairs of target word and actual/fake context word. Each word is put through on embedding layer. Remember that W2V trains two embeddings per word, one when the word is the target word and one when it appears in the context of some other word. So using two embedding layers is important.\n",
    "\n",
    "Having obtained word embeddings for the target and context word, we pass these embeddings to a merge layer in which we compute the dot product of these two vectors. We can think of the dot products as an unnormalized cosine similarity between the two embedding vectors. Put differently, we obtain a similarity score. We want that score to be large when the inputted 'context' word actually appeared in the context of the target word, and small otherwise. Hence, we forward the similarity score to a dense sigmoid layer, which computes a probability of the 'context' word being an actual context word. We then compare this probability, the output of our neural network, to the actual label, which we obtained above from *skipgram()*. Enter back-propagation. \n",
    "\n",
    "So far so good, but there is on issue. Our network is a little more advanced than those be have built so far. There were also some changes when moving to Keras 2., which hit us in this example. Long story short, we cannot use the nice and simple sequential API anymore and will have to use the functional API instead. For this reason, the code will look a little different from what you are used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers import Embedding, Input, Reshape, Dot, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding dimension\n",
    "emd_dim = 25  # relatively small but we do not use much data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up embedding layers for the target and the context word:\n",
    "embedding_target = Embedding(vocab_size, emd_dim, input_length=1, name='embedding_target')\n",
    "embedding_context = Embedding(vocab_size, emd_dim, input_length=1, name='embedding_context')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model architecture using the functional API\n",
    "\n",
    "# Take a single target word\n",
    "input_target = Input((1,))\n",
    "target = embedding_target(input_target)\n",
    "target = Reshape((emd_dim, 1))(target)\n",
    "\n",
    "# Take another word either from the context or a random word from vocabulary\n",
    "input_context = Input((1,))\n",
    "context = embedding_context(input_context)\n",
    "context = Reshape((emd_dim, 1))(context)\n",
    "\n",
    "# Calculate the dot product as an unnormalized cosine distance\n",
    "dot_product = Dot(axes=1, normalize=False)([target, context])\n",
    "dot_product = Reshape((1,))(dot_product)\n",
    "\n",
    "# Predict if the words are in the same context -> Binary yes/no\n",
    "output = Activation(activation='sigmoid')(dot_product)\n",
    "\n",
    "# Compile the model\n",
    "model = Model(inputs=[input_target, input_context], outputs=output)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how the model is not much of a neural network? The only trainable parameters are the embeddings, which are then dot-multiplied. We thus have two hidden layers side-by-side rather than one after the other and no non-linear activation of the hidden layers! This is very similar to matrix factorization and you can use the same architecture to build a collaborative filter on users (one embedding matrix) and items (one embedding matrix). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a maybe more intuitive visualization of the model thanks to [Dipanjan Sarkar.](https://towardsdatascience.com/understanding-feature-engineering-part-4-deep-learning-methods-for-text-data-96c44370bbfa)\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1123/1*4Uil1zWWF5-jlt-FnRJgAQ.png\">\n",
    "<br>\n",
    "Image source: \n",
    "https://miro.medium.com/max/1123/1*4Uil1zWWF5-jlt-FnRJgAQ.png2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train our model review by review, updating the model after every review (i.e., batch). Implementing this approach is not possible when using the standard Keras training loop. Therefore, we use the function *train_on_batch*, which gives us more control over the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of epochs\n",
    "nb_epoch = 5\n",
    "\n",
    "for e in range(nb_epoch):\n",
    "        print('-'*40)\n",
    "        if e>0:\n",
    "            print('Epoch {} elapsed {:.2f} min.'.format(e, (end-start)/60))\n",
    "        else:\n",
    "            print('Epoch {}'.format(e))\n",
    "        print('-'*40)\n",
    "        start = time.time()\n",
    "\n",
    "        samples_seen = 0\n",
    "        losses = []\n",
    "        \n",
    "        for couples, labels in skip_grams:\n",
    "            if couples:\n",
    "                X = np.array(couples, dtype=\"int32\")\n",
    "                loss = model.train_on_batch([X[:,0],X[:,1]], labels)\n",
    "                losses.append(loss)\n",
    "        print(f'Average loss over last 1000 batches: {np.mean(losses[-1000:])}')\n",
    "        end = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the weights\n",
    "We can extract the word embeddings from the corresponding layer of our model. Converting the embeddings to a data frame facilitates a quick look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings = model.get_layer(name=\"embedding_target\").get_weights()[0]\n",
    "print(word_embeddings.shape)\n",
    "w2v_df = pd.DataFrame(word_embeddings, index=id2word.values())\n",
    "w2v_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try to reproduce some of the functionality demonstrated above for the Gensim implementation. Of course, we don't go all the way. Still, doing a little similarity calculation is not too difficult. We use some scikit-learn functionality to create a matrix of pairwise distances between words. We can then query the most similar words to some seed-words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "distance_matrix = euclidean_distances(word_embeddings)\n",
    "print(distance_matrix.shape)\n",
    "\n",
    "# Note that this code will not work if you trained on a small corpus\n",
    "# To make it work, you have to ensure that your search words are part of the vocabulary.\n",
    "similar_words = {search_term: [id2word[idx] for idx in distance_matrix[word2id[search_term]-1].argsort()[1:6]+1] \n",
    "                   for search_term in ['tarantino', 'cruise', 'willis', 'lawrence', 'bullock']}\n",
    "\n",
    "similar_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, we might want to continue training our embeddings.   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (adams)",
   "language": "python",
   "name": "pycharm-feb95198"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
